D:\softwares\anaconda3\envs\RGAT20230627\python.exe "D:\softwares\Pycharm\PyCharm Community Edition 2022.1.3\plugins\python-ce\helpers\pydev\pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 2867 --file F:/Code/240313/gat/graph/gcn_gat_link_weight_node_b/main_mydata.py
Connected to pydev debugger (build 221.5921.27)
Loading zh dataset...
D:\softwares\anaconda3\envs\RGAT20230627\lib\site-packages\torch_geometric\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead
  warnings.warn(out)
Epoch: 001, Loss: 0.6534, Val: 0.5608, Test: 0.4985
Epoch: 002, Loss: 0.6945, Val: 0.5111, Test: 0.4985
Epoch: 003, Loss: 0.6509, Val: 0.5800, Test: 0.5917
Epoch: 004, Loss: 0.6651, Val: 0.4757, Test: 0.5917
Epoch: 005, Loss: 0.6553, Val: 0.5062, Test: 0.5917
Epoch: 006, Loss: 0.6482, Val: 0.5003, Test: 0.5917
Epoch: 007, Loss: 0.6414, Val: 0.7374, Test: 0.6706
Epoch: 008, Loss: 0.6203, Val: 0.7788, Test: 0.6314
Epoch: 009, Loss: 0.6513, Val: 0.6766, Test: 0.6314
Epoch: 010, Loss: 0.6083, Val: 0.6739, Test: 0.6314
Epoch: 011, Loss: 0.6024, Val: 0.5425, Test: 0.6314
Epoch: 012, Loss: 0.5964, Val: 0.5030, Test: 0.6314
Epoch: 013, Loss: 0.5741, Val: 0.5219, Test: 0.6314
Epoch: 014, Loss: 0.5800, Val: 0.5327, Test: 0.6314
Epoch: 015, Loss: 0.5772, Val: 0.5176, Test: 0.6314
Epoch: 016, Loss: 0.5525, Val: 0.5700, Test: 0.6314
Epoch: 017, Loss: 0.5590, Val: 0.6560, Test: 0.6314
Epoch: 018, Loss: 0.5500, Val: 0.5809, Test: 0.6314
Epoch: 019, Loss: 0.5435, Val: 0.5987, Test: 0.6314
Epoch: 020, Loss: 0.5876, Val: 0.5863, Test: 0.6314
Epoch: 021, Loss: 0.5365, Val: 0.6533, Test: 0.6314
Epoch: 022, Loss: 0.5529, Val: 0.6944, Test: 0.6314
Epoch: 023, Loss: 0.5492, Val: 0.6036, Test: 0.6314
Epoch: 024, Loss: 0.5548, Val: 0.5663, Test: 0.6314
Epoch: 025, Loss: 0.5533, Val: 0.5895, Test: 0.6314
Epoch: 026, Loss: 0.5544, Val: 0.7047, Test: 0.6314
Epoch: 027, Loss: 0.5601, Val: 0.7388, Test: 0.6314
Epoch: 028, Loss: 0.5515, Val: 0.6750, Test: 0.6314
Epoch: 029, Loss: 0.5361, Val: 0.6009, Test: 0.6314
Epoch: 030, Loss: 0.5460, Val: 0.5895, Test: 0.6314
Epoch: 031, Loss: 0.5305, Val: 0.6063, Test: 0.6314
Epoch: 032, Loss: 0.5357, Val: 0.6284, Test: 0.6314
Epoch: 033, Loss: 0.5235, Val: 0.6685, Test: 0.6314
Epoch: 034, Loss: 0.5363, Val: 0.6982, Test: 0.6314
Epoch: 035, Loss: 0.5298, Val: 0.7177, Test: 0.6314
Epoch: 036, Loss: 0.5406, Val: 0.7134, Test: 0.6314
Epoch: 037, Loss: 0.5229, Val: 0.6814, Test: 0.6314
Epoch: 038, Loss: 0.5403, Val: 0.6739, Test: 0.6314
Epoch: 039, Loss: 0.5492, Val: 0.6679, Test: 0.6314
Epoch: 040, Loss: 0.5335, Val: 0.6614, Test: 0.6314
Epoch: 041, Loss: 0.5240, Val: 0.7036, Test: 0.6314
Epoch: 042, Loss: 0.5273, Val: 0.7144, Test: 0.6314
Epoch: 043, Loss: 0.5396, Val: 0.7323, Test: 0.6314
Epoch: 044, Loss: 0.5320, Val: 0.7534, Test: 0.6314
Epoch: 045, Loss: 0.5230, Val: 0.7409, Test: 0.6314
Epoch: 046, Loss: 0.5155, Val: 0.7307, Test: 0.6314
Epoch: 047, Loss: 0.5141, Val: 0.7280, Test: 0.6314
Epoch: 048, Loss: 0.5252, Val: 0.7458, Test: 0.6314
Epoch: 049, Loss: 0.5195, Val: 0.7626, Test: 0.6314
Epoch: 050, Loss: 0.5252, Val: 0.7864, Test: 0.7627
Epoch: 051, Loss: 0.5141, Val: 0.7761, Test: 0.7627
Epoch: 052, Loss: 0.5218, Val: 0.7653, Test: 0.7627
Epoch: 053, Loss: 0.5123, Val: 0.7674, Test: 0.7627
Epoch: 054, Loss: 0.5264, Val: 0.7577, Test: 0.7627
Epoch: 055, Loss: 0.5219, Val: 0.7453, Test: 0.7627
Epoch: 056, Loss: 0.5093, Val: 0.7588, Test: 0.7627
Epoch: 057, Loss: 0.5207, Val: 0.7923, Test: 0.7551
Epoch: 058, Loss: 0.5016, Val: 0.8015, Test: 0.7436
Epoch: 059, Loss: 0.5221, Val: 0.7950, Test: 0.7436
Epoch: 060, Loss: 0.5328, Val: 0.7729, Test: 0.7436
Epoch: 061, Loss: 0.5142, Val: 0.7631, Test: 0.7436
Epoch: 062, Loss: 0.5384, Val: 0.7934, Test: 0.7436
Epoch: 063, Loss: 0.5237, Val: 0.7918, Test: 0.7436
Epoch: 064, Loss: 0.5360, Val: 0.7847, Test: 0.7436
Epoch: 065, Loss: 0.5124, Val: 0.8042, Test: 0.7435
Epoch: 066, Loss: 0.5220, Val: 0.8037, Test: 0.7435
Epoch: 067, Loss: 0.5074, Val: 0.7734, Test: 0.7435
Epoch: 068, Loss: 0.5165, Val: 0.7345, Test: 0.7435
Epoch: 069, Loss: 0.5194, Val: 0.7458, Test: 0.7435
Epoch: 070, Loss: 0.5091, Val: 0.7701, Test: 0.7435
Epoch: 071, Loss: 0.5165, Val: 0.7707, Test: 0.7435
Epoch: 072, Loss: 0.5163, Val: 0.7961, Test: 0.7435
Epoch: 073, Loss: 0.5123, Val: 0.8091, Test: 0.7524
Epoch: 074, Loss: 0.5188, Val: 0.8102, Test: 0.7588
Epoch: 075, Loss: 0.5017, Val: 0.7956, Test: 0.7588
Epoch: 076, Loss: 0.5092, Val: 0.7355, Test: 0.7588
Epoch: 077, Loss: 0.5281, Val: 0.7247, Test: 0.7588
Epoch: 078, Loss: 0.5218, Val: 0.7799, Test: 0.7588
Epoch: 079, Loss: 0.5179, Val: 0.7950, Test: 0.7588
Epoch: 080, Loss: 0.5248, Val: 0.7896, Test: 0.7588
Epoch: 081, Loss: 0.5319, Val: 0.7896, Test: 0.7588
Epoch: 082, Loss: 0.5222, Val: 0.7572, Test: 0.7588
Epoch: 083, Loss: 0.5141, Val: 0.7377, Test: 0.7588
Epoch: 084, Loss: 0.5254, Val: 0.7831, Test: 0.7588
Epoch: 085, Loss: 0.5188, Val: 0.8134, Test: 0.7628
Epoch: 086, Loss: 0.5119, Val: 0.8183, Test: 0.7664
Epoch: 087, Loss: 0.5213, Val: 0.8080, Test: 0.7664
Epoch: 088, Loss: 0.4985, Val: 0.7929, Test: 0.7664
Epoch: 089, Loss: 0.5139, Val: 0.7756, Test: 0.7664
Epoch: 090, Loss: 0.5162, Val: 0.7831, Test: 0.7664
Epoch: 091, Loss: 0.5193, Val: 0.7983, Test: 0.7664
Epoch: 092, Loss: 0.5013, Val: 0.8075, Test: 0.7664
Epoch: 093, Loss: 0.5177, Val: 0.8102, Test: 0.7664
Epoch: 094, Loss: 0.5044, Val: 0.8102, Test: 0.7664
Epoch: 095, Loss: 0.5019, Val: 0.8048, Test: 0.7664
Epoch: 096, Loss: 0.5112, Val: 0.7864, Test: 0.7664
Epoch: 097, Loss: 0.5009, Val: 0.7891, Test: 0.7664
Epoch: 098, Loss: 0.5077, Val: 0.8048, Test: 0.7664
Epoch: 099, Loss: 0.5074, Val: 0.8096, Test: 0.7664
Epoch: 100, Loss: 0.5119, Val: 0.8177, Test: 0.7664
Backend QtAgg is interactive backend. Turning interactive mode on.
Python 3.8.17 (default, Jul  5 2023, 20:35:33) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.12.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 8.12.0



Epoch: 013, Loss: 0.5296, acc: 0.8206, hamming_loss: 0.1794, Val: 0.8626, Test: 0.8856

Process finished with exit code 0


D:\softwares\anaconda3\envs\RGAT20230627\python.exe "D:\softwares\Pycharm\PyCharm Community Edition 2022.1.3\plugins\python-ce\helpers\pydev\pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 12239 --file F:/Code/240313/gat/graph/gcn_gat_link_weight_node_b/load_model.py
Connected to pydev debugger (build 221.5921.27)
Loading zh dataset...
D:\softwares\anaconda3\envs\RGAT20230627\lib\site-packages\torch_geometric\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead
  warnings.warn(out)
Epoch: 001, Loss: 0.5070, acc: 0.8444, hamming_loss: 0.1556, Val: 0.6133, Test: 0.6936
Epoch: 002, Loss: 0.7080, acc: 0.7332, hamming_loss: 0.2668, Val: 0.8037, Test: 0.8072
Epoch: 003, Loss: 0.5627, acc: 0.8203, hamming_loss: 0.1797, Val: 0.7658, Test: 0.8072
Epoch: 004, Loss: 0.5710, acc: 0.8069, hamming_loss: 0.1931, Val: 0.6912, Test: 0.8072
Epoch: 005, Loss: 0.5691, acc: 0.8065, hamming_loss: 0.1935, Val: 0.6906, Test: 0.8072
Epoch: 006, Loss: 0.6016, acc: 0.7923, hamming_loss: 0.2077, Val: 0.7610, Test: 0.8072
Epoch: 007, Loss: 0.5724, acc: 0.8167, hamming_loss: 0.1833, Val: 0.8112, Test: 0.7365
Epoch: 008, Loss: 0.5583, acc: 0.8364, hamming_loss: 0.1636, Val: 0.8345, Test: 0.7086
Epoch: 009, Loss: 0.5350, acc: 0.8535, hamming_loss: 0.1465, Val: 0.8318, Test: 0.7086
Epoch: 010, Loss: 0.5396, acc: 0.8484, hamming_loss: 0.1516, Val: 0.8210, Test: 0.7086
Epoch: 011, Loss: 0.5441, acc: 0.8262, hamming_loss: 0.1738, Val: 0.7685, Test: 0.7086
Epoch: 012, Loss: 0.5444, acc: 0.8229, hamming_loss: 0.1771, Val: 0.7420, Test: 0.7086
Epoch: 013, Loss: 0.5500, acc: 0.8141, hamming_loss: 0.1859, Val: 0.7658, Test: 0.7086
Epoch: 014, Loss: 0.5514, acc: 0.8207, hamming_loss: 0.1793, Val: 0.8096, Test: 0.7086
Epoch: 015, Loss: 0.5542, acc: 0.8120, hamming_loss: 0.1880, Val: 0.8367, Test: 0.7969
Epoch: 016, Loss: 0.5533, acc: 0.8181, hamming_loss: 0.1819, Val: 0.8475, Test: 0.7524
Epoch: 017, Loss: 0.5454, acc: 0.8189, hamming_loss: 0.1811, Val: 0.8361, Test: 0.7524
Epoch: 018, Loss: 0.5282, acc: 0.8444, hamming_loss: 0.1556, Val: 0.8296, Test: 0.7524
Epoch: 019, Loss: 0.5123, acc: 0.8568, hamming_loss: 0.1432, Val: 0.8156, Test: 0.7524
Epoch: 020, Loss: 0.5364, acc: 0.8233, hamming_loss: 0.1767, Val: 0.7999, Test: 0.7524
Epoch: 021, Loss: 0.5301, acc: 0.8484, hamming_loss: 0.1516, Val: 0.7588, Test: 0.7524
Epoch: 022, Loss: 0.5268, acc: 0.8356, hamming_loss: 0.1644, Val: 0.7426, Test: 0.7524
Epoch: 023, Loss: 0.5412, acc: 0.8181, hamming_loss: 0.1819, Val: 0.7620, Test: 0.7524
Epoch: 024, Loss: 0.5236, acc: 0.8364, hamming_loss: 0.1636, Val: 0.7994, Test: 0.7524
Epoch: 025, Loss: 0.5259, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8378, Test: 0.7524
Epoch: 026, Loss: 0.5038, acc: 0.8473, hamming_loss: 0.1527, Val: 0.8594, Test: 0.7608
Epoch: 027, Loss: 0.5176, acc: 0.8433, hamming_loss: 0.1567, Val: 0.8507, Test: 0.7608
Epoch: 028, Loss: 0.5103, acc: 0.8400, hamming_loss: 0.1600, Val: 0.8334, Test: 0.7608
Epoch: 029, Loss: 0.5205, acc: 0.8327, hamming_loss: 0.1673, Val: 0.8085, Test: 0.7608
Epoch: 030, Loss: 0.5120, acc: 0.8371, hamming_loss: 0.1629, Val: 0.8037, Test: 0.7608
Epoch: 031, Loss: 0.5183, acc: 0.8309, hamming_loss: 0.1691, Val: 0.7988, Test: 0.7608
Epoch: 032, Loss: 0.5069, acc: 0.8429, hamming_loss: 0.1571, Val: 0.7977, Test: 0.7608
Epoch: 033, Loss: 0.5167, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8131, Test: 0.7608
Epoch: 034, Loss: 0.5084, acc: 0.8451, hamming_loss: 0.1549, Val: 0.8388, Test: 0.7608
Epoch: 035, Loss: 0.5062, acc: 0.8502, hamming_loss: 0.1498, Val: 0.8615, Test: 0.7816
Epoch: 036, Loss: 0.5044, acc: 0.8316, hamming_loss: 0.1684, Val: 0.8772, Test: 0.7895
Epoch: 037, Loss: 0.5111, acc: 0.8451, hamming_loss: 0.1549, Val: 0.8740, Test: 0.7895
Epoch: 038, Loss: 0.4992, acc: 0.8491, hamming_loss: 0.1509, Val: 0.8480, Test: 0.7895
Epoch: 039, Loss: 0.5064, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8264, Test: 0.7895
Epoch: 040, Loss: 0.5113, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8264, Test: 0.7895
Epoch: 041, Loss: 0.5133, acc: 0.8338, hamming_loss: 0.1662, Val: 0.8286, Test: 0.7895
Epoch: 042, Loss: 0.5083, acc: 0.8422, hamming_loss: 0.1578, Val: 0.8388, Test: 0.7895
Epoch: 043, Loss: 0.5053, acc: 0.8298, hamming_loss: 0.1702, Val: 0.8486, Test: 0.7895
Epoch: 044, Loss: 0.4976, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8486, Test: 0.7895
Epoch: 045, Loss: 0.4951, acc: 0.8520, hamming_loss: 0.1480, Val: 0.8378, Test: 0.7895
Epoch: 046, Loss: 0.5120, acc: 0.8451, hamming_loss: 0.1549, Val: 0.8491, Test: 0.7895
Epoch: 047, Loss: 0.5070, acc: 0.8462, hamming_loss: 0.1538, Val: 0.8496, Test: 0.7895
Epoch: 048, Loss: 0.5022, acc: 0.8499, hamming_loss: 0.1501, Val: 0.8378, Test: 0.7895
Epoch: 049, Loss: 0.5031, acc: 0.8480, hamming_loss: 0.1520, Val: 0.8323, Test: 0.7895
Epoch: 050, Loss: 0.5191, acc: 0.8426, hamming_loss: 0.1574, Val: 0.8437, Test: 0.7895
Epoch: 051, Loss: 0.5095, acc: 0.8499, hamming_loss: 0.1501, Val: 0.8453, Test: 0.7895
Epoch: 052, Loss: 0.5101, acc: 0.8345, hamming_loss: 0.1655, Val: 0.8442, Test: 0.7895
Epoch: 053, Loss: 0.5067, acc: 0.8389, hamming_loss: 0.1611, Val: 0.8518, Test: 0.7895
Epoch: 054, Loss: 0.5133, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8605, Test: 0.7895
Epoch: 055, Loss: 0.5027, acc: 0.8458, hamming_loss: 0.1542, Val: 0.8534, Test: 0.7895
Epoch: 056, Loss: 0.5037, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8394, Test: 0.7895
Epoch: 057, Loss: 0.5053, acc: 0.8557, hamming_loss: 0.1443, Val: 0.8432, Test: 0.7895
Epoch: 058, Loss: 0.5000, acc: 0.8517, hamming_loss: 0.1483, Val: 0.8415, Test: 0.7895
Epoch: 059, Loss: 0.5015, acc: 0.8546, hamming_loss: 0.1454, Val: 0.8361, Test: 0.7895
Epoch: 060, Loss: 0.5050, acc: 0.8448, hamming_loss: 0.1552, Val: 0.8367, Test: 0.7895
Epoch: 061, Loss: 0.4996, acc: 0.8499, hamming_loss: 0.1501, Val: 0.8394, Test: 0.7895
Epoch: 062, Loss: 0.5207, acc: 0.8389, hamming_loss: 0.1611, Val: 0.8432, Test: 0.7895
Epoch: 063, Loss: 0.5063, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8464, Test: 0.7895
Epoch: 064, Loss: 0.5164, acc: 0.8360, hamming_loss: 0.1640, Val: 0.8518, Test: 0.7895
Epoch: 065, Loss: 0.5013, acc: 0.8550, hamming_loss: 0.1450, Val: 0.8502, Test: 0.7895
Epoch: 066, Loss: 0.5058, acc: 0.8520, hamming_loss: 0.1480, Val: 0.8496, Test: 0.7895
Epoch: 067, Loss: 0.5387, acc: 0.8236, hamming_loss: 0.1764, Val: 0.8475, Test: 0.7895
Epoch: 068, Loss: 0.5162, acc: 0.8345, hamming_loss: 0.1655, Val: 0.8561, Test: 0.7895
Epoch: 069, Loss: 0.5043, acc: 0.8502, hamming_loss: 0.1498, Val: 0.8394, Test: 0.7895
Epoch: 070, Loss: 0.4931, acc: 0.8495, hamming_loss: 0.1505, Val: 0.8264, Test: 0.7895
Epoch: 071, Loss: 0.5067, acc: 0.8422, hamming_loss: 0.1578, Val: 0.8210, Test: 0.7895
Epoch: 072, Loss: 0.5073, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8340, Test: 0.7895
Epoch: 073, Loss: 0.4997, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8291, Test: 0.7895
Epoch: 074, Loss: 0.4912, acc: 0.8663, hamming_loss: 0.1337, Val: 0.8291, Test: 0.7895
Epoch: 075, Loss: 0.5091, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8464, Test: 0.7895
Epoch: 076, Loss: 0.4887, acc: 0.8673, hamming_loss: 0.1327, Val: 0.8605, Test: 0.7895
Epoch: 077, Loss: 0.4989, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8610, Test: 0.7895
Epoch: 078, Loss: 0.5066, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8480, Test: 0.7895
Epoch: 079, Loss: 0.4979, acc: 0.8531, hamming_loss: 0.1469, Val: 0.8194, Test: 0.7895
Epoch: 080, Loss: 0.4937, acc: 0.8560, hamming_loss: 0.1440, Val: 0.7923, Test: 0.7895
Epoch: 081, Loss: 0.5044, acc: 0.8477, hamming_loss: 0.1523, Val: 0.7988, Test: 0.7895
Epoch: 082, Loss: 0.5084, acc: 0.8535, hamming_loss: 0.1465, Val: 0.8442, Test: 0.7895
Epoch: 083, Loss: 0.4964, acc: 0.8302, hamming_loss: 0.1698, Val: 0.8643, Test: 0.7895
Epoch: 084, Loss: 0.5030, acc: 0.8433, hamming_loss: 0.1567, Val: 0.8632, Test: 0.7895
Epoch: 085, Loss: 0.4873, acc: 0.8484, hamming_loss: 0.1516, Val: 0.8664, Test: 0.7895
Epoch: 086, Loss: 0.4958, acc: 0.8473, hamming_loss: 0.1527, Val: 0.8551, Test: 0.7895
Epoch: 087, Loss: 0.4876, acc: 0.8560, hamming_loss: 0.1440, Val: 0.8134, Test: 0.7895
Epoch: 088, Loss: 0.4980, acc: 0.8437, hamming_loss: 0.1563, Val: 0.7956, Test: 0.7895
Epoch: 089, Loss: 0.4998, acc: 0.8495, hamming_loss: 0.1505, Val: 0.8102, Test: 0.7895
Epoch: 090, Loss: 0.4961, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8383, Test: 0.7895
Epoch: 091, Loss: 0.4954, acc: 0.8426, hamming_loss: 0.1574, Val: 0.8513, Test: 0.7895
Epoch: 092, Loss: 0.4916, acc: 0.8411, hamming_loss: 0.1589, Val: 0.8578, Test: 0.7895
Epoch: 093, Loss: 0.4970, acc: 0.8437, hamming_loss: 0.1563, Val: 0.8529, Test: 0.7895
Epoch: 094, Loss: 0.4901, acc: 0.8448, hamming_loss: 0.1552, Val: 0.8383, Test: 0.7895
Epoch: 095, Loss: 0.5033, acc: 0.8502, hamming_loss: 0.1498, Val: 0.8356, Test: 0.7895
Epoch: 096, Loss: 0.4884, acc: 0.8437, hamming_loss: 0.1563, Val: 0.8421, Test: 0.7895
Epoch: 097, Loss: 0.4802, acc: 0.8648, hamming_loss: 0.1352, Val: 0.8513, Test: 0.7895
Epoch: 098, Loss: 0.4906, acc: 0.8455, hamming_loss: 0.1545, Val: 0.8518, Test: 0.7895
Epoch: 099, Loss: 0.5127, acc: 0.8400, hamming_loss: 0.1600, Val: 0.8524, Test: 0.7895
Epoch: 100, Loss: 0.4836, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8399, Test: 0.7895
Backend QtAgg is interactive backend. Turning interactive mode on.


D:\softwares\anaconda3\envs\RGAT20230627\python.exe "D:\softwares\Pycharm\PyCharm Community Edition 2022.1.3\plugins\python-ce\helpers\pydev\pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 9109 --file F:/Code/240313/gat/graph/gcn_gat_link_weight_node_b/load_model.py
Connected to pydev debugger (build 221.5921.27)
Loading zh dataset...
D:\softwares\anaconda3\envs\RGAT20230627\lib\site-packages\torch_geometric\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead
  warnings.warn(out)
Epoch: 001, Loss: 0.5200, acc: 0.8317, hamming_loss: 0.1683,f1: 0.8305, Val: 0.6122, Test: 0.7016
Epoch: 002, Loss: 0.7738, acc: 0.7221, hamming_loss: 0.2779,f1: 0.6994, Val: 0.7810, Test: 0.8291
Epoch: 003, Loss: 0.5421, acc: 0.8288, hamming_loss: 0.1712,f1: 0.8261, Val: 0.7188, Test: 0.8291
Epoch: 004, Loss: 0.5669, acc: 0.8178, hamming_loss: 0.1822,f1: 0.8144, Val: 0.7052, Test: 0.8291
Epoch: 005, Loss: 0.5991, acc: 0.7859, hamming_loss: 0.2141,f1: 0.7799, Val: 0.7069, Test: 0.8291
Epoch: 006, Loss: 0.6118, acc: 0.7881, hamming_loss: 0.2119,f1: 0.7797, Val: 0.7512, Test: 0.8291
Epoch: 007, Loss: 0.6090, acc: 0.7892, hamming_loss: 0.2108,f1: 0.7841, Val: 0.7453, Test: 0.8291
Epoch: 008, Loss: 0.5849, acc: 0.8127, hamming_loss: 0.1873,f1: 0.8087, Val: 0.7263, Test: 0.8291
Epoch: 009, Loss: 0.5750, acc: 0.8189, hamming_loss: 0.1811,f1: 0.8176, Val: 0.7085, Test: 0.8291
Epoch: 010, Loss: 0.5524, acc: 0.8317, hamming_loss: 0.1683,f1: 0.8315, Val: 0.6928, Test: 0.8291
Epoch: 011, Loss: 0.5477, acc: 0.8317, hamming_loss: 0.1683,f1: 0.8317, Val: 0.6869, Test: 0.8291
Epoch: 012, Loss: 0.5386, acc: 0.8218, hamming_loss: 0.1782,f1: 0.8218, Val: 0.6906, Test: 0.8291
Epoch: 013, Loss: 0.5368, acc: 0.8229, hamming_loss: 0.1771,f1: 0.8227, Val: 0.7074, Test: 0.8291
Epoch: 014, Loss: 0.5346, acc: 0.8255, hamming_loss: 0.1745,f1: 0.8252, Val: 0.7269, Test: 0.8291
Epoch: 015, Loss: 0.5534, acc: 0.8240, hamming_loss: 0.1760,f1: 0.8239, Val: 0.7474, Test: 0.8291
Epoch: 016, Loss: 0.5370, acc: 0.8306, hamming_loss: 0.1694,f1: 0.8303, Val: 0.7577, Test: 0.8291
Epoch: 017, Loss: 0.5445, acc: 0.8226, hamming_loss: 0.1774,f1: 0.8224, Val: 0.7528, Test: 0.8291
Epoch: 018, Loss: 0.5358, acc: 0.8255, hamming_loss: 0.1745,f1: 0.8252, Val: 0.7420, Test: 0.8291
Epoch: 019, Loss: 0.5596, acc: 0.8065, hamming_loss: 0.1935,f1: 0.8058, Val: 0.7296, Test: 0.8291
Epoch: 020, Loss: 0.5349, acc: 0.8226, hamming_loss: 0.1774,f1: 0.8221, Val: 0.7052, Test: 0.8291
Epoch: 021, Loss: 0.5442, acc: 0.8343, hamming_loss: 0.1657,f1: 0.8340, Val: 0.6977, Test: 0.8291
Epoch: 022, Loss: 0.5434, acc: 0.8120, hamming_loss: 0.1880,f1: 0.8118, Val: 0.6858, Test: 0.8291
Epoch: 023, Loss: 0.5236, acc: 0.8449, hamming_loss: 0.1551,f1: 0.8443, Val: 0.6863, Test: 0.8291
Epoch: 024, Loss: 0.5290, acc: 0.8479, hamming_loss: 0.1521,f1: 0.8471, Val: 0.6944, Test: 0.8291
Epoch: 025, Loss: 0.5111, acc: 0.8376, hamming_loss: 0.1624,f1: 0.8367, Val: 0.7085, Test: 0.8291
Epoch: 026, Loss: 0.5148, acc: 0.8453, hamming_loss: 0.1547,f1: 0.8445, Val: 0.7274, Test: 0.8291
Epoch: 027, Loss: 0.5186, acc: 0.8413, hamming_loss: 0.1587,f1: 0.8400, Val: 0.7220, Test: 0.8291
Epoch: 028, Loss: 0.5097, acc: 0.8343, hamming_loss: 0.1657,f1: 0.8333, Val: 0.7269, Test: 0.8291
Epoch: 029, Loss: 0.5305, acc: 0.8273, hamming_loss: 0.1727,f1: 0.8263, Val: 0.7355, Test: 0.8291
Epoch: 030, Loss: 0.5355, acc: 0.8350, hamming_loss: 0.1650,f1: 0.8333, Val: 0.7339, Test: 0.8291
Epoch: 031, Loss: 0.5257, acc: 0.8446, hamming_loss: 0.1554,f1: 0.8434, Val: 0.7242, Test: 0.8291
Epoch: 032, Loss: 0.5134, acc: 0.8490, hamming_loss: 0.1510,f1: 0.8476, Val: 0.7128, Test: 0.8291
Epoch: 033, Loss: 0.5155, acc: 0.8380, hamming_loss: 0.1620,f1: 0.8366, Val: 0.7123, Test: 0.8291
Epoch: 034, Loss: 0.5069, acc: 0.8468, hamming_loss: 0.1532,f1: 0.8457, Val: 0.7177, Test: 0.8291
Epoch: 035, Loss: 0.5110, acc: 0.8431, hamming_loss: 0.1569,f1: 0.8418, Val: 0.7280, Test: 0.8291
Epoch: 036, Loss: 0.5277, acc: 0.8453, hamming_loss: 0.1547,f1: 0.8434, Val: 0.7350, Test: 0.8291
Epoch: 037, Loss: 0.5005, acc: 0.8530, hamming_loss: 0.1470,f1: 0.8518, Val: 0.7480, Test: 0.8291
Epoch: 038, Loss: 0.5247, acc: 0.8204, hamming_loss: 0.1796,f1: 0.8186, Val: 0.7550, Test: 0.8291
Epoch: 039, Loss: 0.5169, acc: 0.8361, hamming_loss: 0.1639,f1: 0.8349, Val: 0.7545, Test: 0.8291
Epoch: 040, Loss: 0.5023, acc: 0.8431, hamming_loss: 0.1569,f1: 0.8416, Val: 0.7458, Test: 0.8291
Epoch: 041, Loss: 0.5127, acc: 0.8534, hamming_loss: 0.1466,f1: 0.8516, Val: 0.7469, Test: 0.8291
Epoch: 042, Loss: 0.5293, acc: 0.8361, hamming_loss: 0.1639,f1: 0.8343, Val: 0.7523, Test: 0.8291
Epoch: 043, Loss: 0.5135, acc: 0.8299, hamming_loss: 0.1701,f1: 0.8275, Val: 0.7496, Test: 0.8291
Epoch: 044, Loss: 0.5130, acc: 0.8347, hamming_loss: 0.1653,f1: 0.8324, Val: 0.7491, Test: 0.8291
Epoch: 045, Loss: 0.5120, acc: 0.8380, hamming_loss: 0.1620,f1: 0.8361, Val: 0.7512, Test: 0.8291
Epoch: 046, Loss: 0.5057, acc: 0.8435, hamming_loss: 0.1565,f1: 0.8420, Val: 0.7626, Test: 0.8291
Epoch: 047, Loss: 0.5174, acc: 0.8358, hamming_loss: 0.1642,f1: 0.8338, Val: 0.7734, Test: 0.8291
Epoch: 048, Loss: 0.5168, acc: 0.8413, hamming_loss: 0.1587,f1: 0.8393, Val: 0.7858, Test: 0.8191
Epoch: 049, Loss: 0.4921, acc: 0.8581, hamming_loss: 0.1419,f1: 0.8567, Val: 0.7853, Test: 0.8191
Epoch: 050, Loss: 0.5025, acc: 0.8369, hamming_loss: 0.1631,f1: 0.8351, Val: 0.7799, Test: 0.8191
Epoch: 051, Loss: 0.5113, acc: 0.8501, hamming_loss: 0.1499,f1: 0.8485, Val: 0.7831, Test: 0.8191
Epoch: 052, Loss: 0.5215, acc: 0.8354, hamming_loss: 0.1646,f1: 0.8333, Val: 0.7718, Test: 0.8191
Epoch: 053, Loss: 0.5063, acc: 0.8526, hamming_loss: 0.1474,f1: 0.8506, Val: 0.7691, Test: 0.8191
Epoch: 054, Loss: 0.5190, acc: 0.8325, hamming_loss: 0.1675,f1: 0.8312, Val: 0.7729, Test: 0.8191
Epoch: 055, Loss: 0.5193, acc: 0.8435, hamming_loss: 0.1565,f1: 0.8414, Val: 0.7820, Test: 0.8191
Epoch: 056, Loss: 0.5022, acc: 0.8475, hamming_loss: 0.1525,f1: 0.8462, Val: 0.7907, Test: 0.8127
Epoch: 057, Loss: 0.5121, acc: 0.8427, hamming_loss: 0.1573,f1: 0.8413, Val: 0.8021, Test: 0.8210
Epoch: 058, Loss: 0.4996, acc: 0.8383, hamming_loss: 0.1617,f1: 0.8368, Val: 0.8064, Test: 0.8298
Epoch: 059, Loss: 0.5105, acc: 0.8387, hamming_loss: 0.1613,f1: 0.8371, Val: 0.7923, Test: 0.8298
Epoch: 060, Loss: 0.4861, acc: 0.8405, hamming_loss: 0.1595,f1: 0.8390, Val: 0.7777, Test: 0.8298
Epoch: 061, Loss: 0.4920, acc: 0.8585, hamming_loss: 0.1415,f1: 0.8569, Val: 0.7723, Test: 0.8298
Epoch: 062, Loss: 0.5109, acc: 0.8460, hamming_loss: 0.1540,f1: 0.8442, Val: 0.7707, Test: 0.8298
Epoch: 063, Loss: 0.5115, acc: 0.8464, hamming_loss: 0.1536,f1: 0.8453, Val: 0.7739, Test: 0.8298
Epoch: 064, Loss: 0.5050, acc: 0.8556, hamming_loss: 0.1444,f1: 0.8540, Val: 0.7907, Test: 0.8298
Epoch: 065, Loss: 0.5047, acc: 0.8402, hamming_loss: 0.1598,f1: 0.8380, Val: 0.7988, Test: 0.8298
Epoch: 066, Loss: 0.5083, acc: 0.8387, hamming_loss: 0.1613,f1: 0.8369, Val: 0.7983, Test: 0.8298
Epoch: 067, Loss: 0.5161, acc: 0.8383, hamming_loss: 0.1617,f1: 0.8367, Val: 0.7994, Test: 0.8298
Epoch: 068, Loss: 0.4994, acc: 0.8563, hamming_loss: 0.1437,f1: 0.8542, Val: 0.8085, Test: 0.8242
Epoch: 069, Loss: 0.4813, acc: 0.8541, hamming_loss: 0.1459,f1: 0.8525, Val: 0.8031, Test: 0.8242
Epoch: 070, Loss: 0.5049, acc: 0.8435, hamming_loss: 0.1565,f1: 0.8417, Val: 0.8010, Test: 0.8242
Epoch: 071, Loss: 0.5059, acc: 0.8409, hamming_loss: 0.1591,f1: 0.8395, Val: 0.7983, Test: 0.8242
Epoch: 072, Loss: 0.4993, acc: 0.8493, hamming_loss: 0.1507,f1: 0.8477, Val: 0.7912, Test: 0.8242
Epoch: 073, Loss: 0.5042, acc: 0.8361, hamming_loss: 0.1639,f1: 0.8336, Val: 0.7869, Test: 0.8242
Epoch: 074, Loss: 0.5051, acc: 0.8398, hamming_loss: 0.1602,f1: 0.8381, Val: 0.7804, Test: 0.8242
Epoch: 075, Loss: 0.4982, acc: 0.8534, hamming_loss: 0.1466,f1: 0.8515, Val: 0.7902, Test: 0.8242
Epoch: 076, Loss: 0.4949, acc: 0.8508, hamming_loss: 0.1492,f1: 0.8491, Val: 0.8123, Test: 0.8271
Epoch: 077, Loss: 0.4883, acc: 0.8614, hamming_loss: 0.1386,f1: 0.8600, Val: 0.8253, Test: 0.8277
Epoch: 078, Loss: 0.4927, acc: 0.8475, hamming_loss: 0.1525,f1: 0.8462, Val: 0.8345, Test: 0.8313
Epoch: 079, Loss: 0.4998, acc: 0.8453, hamming_loss: 0.1547,f1: 0.8438, Val: 0.8280, Test: 0.8313
Epoch: 080, Loss: 0.5070, acc: 0.8402, hamming_loss: 0.1598,f1: 0.8384, Val: 0.8183, Test: 0.8313
Epoch: 081, Loss: 0.4923, acc: 0.8603, hamming_loss: 0.1397,f1: 0.8588, Val: 0.7994, Test: 0.8313
Epoch: 082, Loss: 0.4980, acc: 0.8559, hamming_loss: 0.1441,f1: 0.8540, Val: 0.7929, Test: 0.8313
Epoch: 083, Loss: 0.4926, acc: 0.8537, hamming_loss: 0.1463,f1: 0.8519, Val: 0.7994, Test: 0.8313
Epoch: 084, Loss: 0.4915, acc: 0.8493, hamming_loss: 0.1507,f1: 0.8476, Val: 0.8058, Test: 0.8313
Epoch: 085, Loss: 0.4949, acc: 0.8556, hamming_loss: 0.1444,f1: 0.8539, Val: 0.8140, Test: 0.8313
Epoch: 086, Loss: 0.4909, acc: 0.8526, hamming_loss: 0.1474,f1: 0.8510, Val: 0.8199, Test: 0.8313
Epoch: 087, Loss: 0.5088, acc: 0.8361, hamming_loss: 0.1639,f1: 0.8339, Val: 0.8291, Test: 0.8313
Epoch: 088, Loss: 0.4975, acc: 0.8387, hamming_loss: 0.1613,f1: 0.8370, Val: 0.8372, Test: 0.8306
Epoch: 089, Loss: 0.5005, acc: 0.8490, hamming_loss: 0.1510,f1: 0.8469, Val: 0.8367, Test: 0.8306
Epoch: 090, Loss: 0.4906, acc: 0.8512, hamming_loss: 0.1488,f1: 0.8495, Val: 0.8286, Test: 0.8306
Epoch: 091, Loss: 0.5001, acc: 0.8427, hamming_loss: 0.1573,f1: 0.8411, Val: 0.8118, Test: 0.8306
Epoch: 092, Loss: 0.5236, acc: 0.8299, hamming_loss: 0.1701,f1: 0.8272, Val: 0.8053, Test: 0.8306
Epoch: 093, Loss: 0.5079, acc: 0.8391, hamming_loss: 0.1609,f1: 0.8370, Val: 0.8167, Test: 0.8306
Epoch: 094, Loss: 0.4968, acc: 0.8482, hamming_loss: 0.1518,f1: 0.8466, Val: 0.8291, Test: 0.8306
Epoch: 095, Loss: 0.4836, acc: 0.8603, hamming_loss: 0.1397,f1: 0.8590, Val: 0.8378, Test: 0.8315
Epoch: 096, Loss: 0.5087, acc: 0.8339, hamming_loss: 0.1661,f1: 0.8321, Val: 0.8399, Test: 0.8379
Epoch: 097, Loss: 0.5023, acc: 0.8490, hamming_loss: 0.1510,f1: 0.8473, Val: 0.8469, Test: 0.8453
Epoch: 098, Loss: 0.4874, acc: 0.8574, hamming_loss: 0.1426,f1: 0.8560, Val: 0.8356, Test: 0.8453
Epoch: 099, Loss: 0.4925, acc: 0.8563, hamming_loss: 0.1437,f1: 0.8547, Val: 0.8204, Test: 0.8453
Epoch: 100, Loss: 0.4853, acc: 0.8563, hamming_loss: 0.1437,f1: 0.8550, Val: 0.8107, Test: 0.8453
Backend QtAgg is interactive backend. Turning interactive mode on.

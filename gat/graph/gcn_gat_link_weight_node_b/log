D:\softwares\anaconda3\envs\RGAT20230627\python.exe "D:\softwares\Pycharm\PyCharm Community Edition 2022.1.3\plugins\python-ce\helpers\pydev\pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 2867 --file F:/Code/240313/gat/graph/gcn_gat_link_weight_node_b/main_mydata.py
Connected to pydev debugger (build 221.5921.27)
Loading zh dataset...
D:\softwares\anaconda3\envs\RGAT20230627\lib\site-packages\torch_geometric\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead
  warnings.warn(out)
Epoch: 001, Loss: 0.6534, Val: 0.5608, Test: 0.4985
Epoch: 002, Loss: 0.6945, Val: 0.5111, Test: 0.4985
Epoch: 003, Loss: 0.6509, Val: 0.5800, Test: 0.5917
Epoch: 004, Loss: 0.6651, Val: 0.4757, Test: 0.5917
Epoch: 005, Loss: 0.6553, Val: 0.5062, Test: 0.5917
Epoch: 006, Loss: 0.6482, Val: 0.5003, Test: 0.5917
Epoch: 007, Loss: 0.6414, Val: 0.7374, Test: 0.6706
Epoch: 008, Loss: 0.6203, Val: 0.7788, Test: 0.6314
Epoch: 009, Loss: 0.6513, Val: 0.6766, Test: 0.6314
Epoch: 010, Loss: 0.6083, Val: 0.6739, Test: 0.6314
Epoch: 011, Loss: 0.6024, Val: 0.5425, Test: 0.6314
Epoch: 012, Loss: 0.5964, Val: 0.5030, Test: 0.6314
Epoch: 013, Loss: 0.5741, Val: 0.5219, Test: 0.6314
Epoch: 014, Loss: 0.5800, Val: 0.5327, Test: 0.6314
Epoch: 015, Loss: 0.5772, Val: 0.5176, Test: 0.6314
Epoch: 016, Loss: 0.5525, Val: 0.5700, Test: 0.6314
Epoch: 017, Loss: 0.5590, Val: 0.6560, Test: 0.6314
Epoch: 018, Loss: 0.5500, Val: 0.5809, Test: 0.6314
Epoch: 019, Loss: 0.5435, Val: 0.5987, Test: 0.6314
Epoch: 020, Loss: 0.5876, Val: 0.5863, Test: 0.6314
Epoch: 021, Loss: 0.5365, Val: 0.6533, Test: 0.6314
Epoch: 022, Loss: 0.5529, Val: 0.6944, Test: 0.6314
Epoch: 023, Loss: 0.5492, Val: 0.6036, Test: 0.6314
Epoch: 024, Loss: 0.5548, Val: 0.5663, Test: 0.6314
Epoch: 025, Loss: 0.5533, Val: 0.5895, Test: 0.6314
Epoch: 026, Loss: 0.5544, Val: 0.7047, Test: 0.6314
Epoch: 027, Loss: 0.5601, Val: 0.7388, Test: 0.6314
Epoch: 028, Loss: 0.5515, Val: 0.6750, Test: 0.6314
Epoch: 029, Loss: 0.5361, Val: 0.6009, Test: 0.6314
Epoch: 030, Loss: 0.5460, Val: 0.5895, Test: 0.6314
Epoch: 031, Loss: 0.5305, Val: 0.6063, Test: 0.6314
Epoch: 032, Loss: 0.5357, Val: 0.6284, Test: 0.6314
Epoch: 033, Loss: 0.5235, Val: 0.6685, Test: 0.6314
Epoch: 034, Loss: 0.5363, Val: 0.6982, Test: 0.6314
Epoch: 035, Loss: 0.5298, Val: 0.7177, Test: 0.6314
Epoch: 036, Loss: 0.5406, Val: 0.7134, Test: 0.6314
Epoch: 037, Loss: 0.5229, Val: 0.6814, Test: 0.6314
Epoch: 038, Loss: 0.5403, Val: 0.6739, Test: 0.6314
Epoch: 039, Loss: 0.5492, Val: 0.6679, Test: 0.6314
Epoch: 040, Loss: 0.5335, Val: 0.6614, Test: 0.6314
Epoch: 041, Loss: 0.5240, Val: 0.7036, Test: 0.6314
Epoch: 042, Loss: 0.5273, Val: 0.7144, Test: 0.6314
Epoch: 043, Loss: 0.5396, Val: 0.7323, Test: 0.6314
Epoch: 044, Loss: 0.5320, Val: 0.7534, Test: 0.6314
Epoch: 045, Loss: 0.5230, Val: 0.7409, Test: 0.6314
Epoch: 046, Loss: 0.5155, Val: 0.7307, Test: 0.6314
Epoch: 047, Loss: 0.5141, Val: 0.7280, Test: 0.6314
Epoch: 048, Loss: 0.5252, Val: 0.7458, Test: 0.6314
Epoch: 049, Loss: 0.5195, Val: 0.7626, Test: 0.6314
Epoch: 050, Loss: 0.5252, Val: 0.7864, Test: 0.7627
Epoch: 051, Loss: 0.5141, Val: 0.7761, Test: 0.7627
Epoch: 052, Loss: 0.5218, Val: 0.7653, Test: 0.7627
Epoch: 053, Loss: 0.5123, Val: 0.7674, Test: 0.7627
Epoch: 054, Loss: 0.5264, Val: 0.7577, Test: 0.7627
Epoch: 055, Loss: 0.5219, Val: 0.7453, Test: 0.7627
Epoch: 056, Loss: 0.5093, Val: 0.7588, Test: 0.7627
Epoch: 057, Loss: 0.5207, Val: 0.7923, Test: 0.7551
Epoch: 058, Loss: 0.5016, Val: 0.8015, Test: 0.7436
Epoch: 059, Loss: 0.5221, Val: 0.7950, Test: 0.7436
Epoch: 060, Loss: 0.5328, Val: 0.7729, Test: 0.7436
Epoch: 061, Loss: 0.5142, Val: 0.7631, Test: 0.7436
Epoch: 062, Loss: 0.5384, Val: 0.7934, Test: 0.7436
Epoch: 063, Loss: 0.5237, Val: 0.7918, Test: 0.7436
Epoch: 064, Loss: 0.5360, Val: 0.7847, Test: 0.7436
Epoch: 065, Loss: 0.5124, Val: 0.8042, Test: 0.7435
Epoch: 066, Loss: 0.5220, Val: 0.8037, Test: 0.7435
Epoch: 067, Loss: 0.5074, Val: 0.7734, Test: 0.7435
Epoch: 068, Loss: 0.5165, Val: 0.7345, Test: 0.7435
Epoch: 069, Loss: 0.5194, Val: 0.7458, Test: 0.7435
Epoch: 070, Loss: 0.5091, Val: 0.7701, Test: 0.7435
Epoch: 071, Loss: 0.5165, Val: 0.7707, Test: 0.7435
Epoch: 072, Loss: 0.5163, Val: 0.7961, Test: 0.7435
Epoch: 073, Loss: 0.5123, Val: 0.8091, Test: 0.7524
Epoch: 074, Loss: 0.5188, Val: 0.8102, Test: 0.7588
Epoch: 075, Loss: 0.5017, Val: 0.7956, Test: 0.7588
Epoch: 076, Loss: 0.5092, Val: 0.7355, Test: 0.7588
Epoch: 077, Loss: 0.5281, Val: 0.7247, Test: 0.7588
Epoch: 078, Loss: 0.5218, Val: 0.7799, Test: 0.7588
Epoch: 079, Loss: 0.5179, Val: 0.7950, Test: 0.7588
Epoch: 080, Loss: 0.5248, Val: 0.7896, Test: 0.7588
Epoch: 081, Loss: 0.5319, Val: 0.7896, Test: 0.7588
Epoch: 082, Loss: 0.5222, Val: 0.7572, Test: 0.7588
Epoch: 083, Loss: 0.5141, Val: 0.7377, Test: 0.7588
Epoch: 084, Loss: 0.5254, Val: 0.7831, Test: 0.7588
Epoch: 085, Loss: 0.5188, Val: 0.8134, Test: 0.7628
Epoch: 086, Loss: 0.5119, Val: 0.8183, Test: 0.7664
Epoch: 087, Loss: 0.5213, Val: 0.8080, Test: 0.7664
Epoch: 088, Loss: 0.4985, Val: 0.7929, Test: 0.7664
Epoch: 089, Loss: 0.5139, Val: 0.7756, Test: 0.7664
Epoch: 090, Loss: 0.5162, Val: 0.7831, Test: 0.7664
Epoch: 091, Loss: 0.5193, Val: 0.7983, Test: 0.7664
Epoch: 092, Loss: 0.5013, Val: 0.8075, Test: 0.7664
Epoch: 093, Loss: 0.5177, Val: 0.8102, Test: 0.7664
Epoch: 094, Loss: 0.5044, Val: 0.8102, Test: 0.7664
Epoch: 095, Loss: 0.5019, Val: 0.8048, Test: 0.7664
Epoch: 096, Loss: 0.5112, Val: 0.7864, Test: 0.7664
Epoch: 097, Loss: 0.5009, Val: 0.7891, Test: 0.7664
Epoch: 098, Loss: 0.5077, Val: 0.8048, Test: 0.7664
Epoch: 099, Loss: 0.5074, Val: 0.8096, Test: 0.7664
Epoch: 100, Loss: 0.5119, Val: 0.8177, Test: 0.7664
Backend QtAgg is interactive backend. Turning interactive mode on.
Python 3.8.17 (default, Jul  5 2023, 20:35:33) [MSC v.1916 64 bit (AMD64)]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.12.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 8.12.0



Epoch: 013, Loss: 0.5296, acc: 0.8206, hamming_loss: 0.1794, Val: 0.8626, Test: 0.8856

Process finished with exit code 0


D:\softwares\anaconda3\envs\RGAT20230627\python.exe "D:\softwares\Pycharm\PyCharm Community Edition 2022.1.3\plugins\python-ce\helpers\pydev\pydevd.py" --multiprocess --qt-support=auto --client 127.0.0.1 --port 12239 --file F:/Code/240313/gat/graph/gcn_gat_link_weight_node_b/load_model.py
Connected to pydev debugger (build 221.5921.27)
Loading zh dataset...
D:\softwares\anaconda3\envs\RGAT20230627\lib\site-packages\torch_geometric\deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead
  warnings.warn(out)
Epoch: 001, Loss: 0.5070, acc: 0.8444, hamming_loss: 0.1556, Val: 0.6133, Test: 0.6936
Epoch: 002, Loss: 0.7080, acc: 0.7332, hamming_loss: 0.2668, Val: 0.8037, Test: 0.8072
Epoch: 003, Loss: 0.5627, acc: 0.8203, hamming_loss: 0.1797, Val: 0.7658, Test: 0.8072
Epoch: 004, Loss: 0.5710, acc: 0.8069, hamming_loss: 0.1931, Val: 0.6912, Test: 0.8072
Epoch: 005, Loss: 0.5691, acc: 0.8065, hamming_loss: 0.1935, Val: 0.6906, Test: 0.8072
Epoch: 006, Loss: 0.6016, acc: 0.7923, hamming_loss: 0.2077, Val: 0.7610, Test: 0.8072
Epoch: 007, Loss: 0.5724, acc: 0.8167, hamming_loss: 0.1833, Val: 0.8112, Test: 0.7365
Epoch: 008, Loss: 0.5583, acc: 0.8364, hamming_loss: 0.1636, Val: 0.8345, Test: 0.7086
Epoch: 009, Loss: 0.5350, acc: 0.8535, hamming_loss: 0.1465, Val: 0.8318, Test: 0.7086
Epoch: 010, Loss: 0.5396, acc: 0.8484, hamming_loss: 0.1516, Val: 0.8210, Test: 0.7086
Epoch: 011, Loss: 0.5441, acc: 0.8262, hamming_loss: 0.1738, Val: 0.7685, Test: 0.7086
Epoch: 012, Loss: 0.5444, acc: 0.8229, hamming_loss: 0.1771, Val: 0.7420, Test: 0.7086
Epoch: 013, Loss: 0.5500, acc: 0.8141, hamming_loss: 0.1859, Val: 0.7658, Test: 0.7086
Epoch: 014, Loss: 0.5514, acc: 0.8207, hamming_loss: 0.1793, Val: 0.8096, Test: 0.7086
Epoch: 015, Loss: 0.5542, acc: 0.8120, hamming_loss: 0.1880, Val: 0.8367, Test: 0.7969
Epoch: 016, Loss: 0.5533, acc: 0.8181, hamming_loss: 0.1819, Val: 0.8475, Test: 0.7524
Epoch: 017, Loss: 0.5454, acc: 0.8189, hamming_loss: 0.1811, Val: 0.8361, Test: 0.7524
Epoch: 018, Loss: 0.5282, acc: 0.8444, hamming_loss: 0.1556, Val: 0.8296, Test: 0.7524
Epoch: 019, Loss: 0.5123, acc: 0.8568, hamming_loss: 0.1432, Val: 0.8156, Test: 0.7524
Epoch: 020, Loss: 0.5364, acc: 0.8233, hamming_loss: 0.1767, Val: 0.7999, Test: 0.7524
Epoch: 021, Loss: 0.5301, acc: 0.8484, hamming_loss: 0.1516, Val: 0.7588, Test: 0.7524
Epoch: 022, Loss: 0.5268, acc: 0.8356, hamming_loss: 0.1644, Val: 0.7426, Test: 0.7524
Epoch: 023, Loss: 0.5412, acc: 0.8181, hamming_loss: 0.1819, Val: 0.7620, Test: 0.7524
Epoch: 024, Loss: 0.5236, acc: 0.8364, hamming_loss: 0.1636, Val: 0.7994, Test: 0.7524
Epoch: 025, Loss: 0.5259, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8378, Test: 0.7524
Epoch: 026, Loss: 0.5038, acc: 0.8473, hamming_loss: 0.1527, Val: 0.8594, Test: 0.7608
Epoch: 027, Loss: 0.5176, acc: 0.8433, hamming_loss: 0.1567, Val: 0.8507, Test: 0.7608
Epoch: 028, Loss: 0.5103, acc: 0.8400, hamming_loss: 0.1600, Val: 0.8334, Test: 0.7608
Epoch: 029, Loss: 0.5205, acc: 0.8327, hamming_loss: 0.1673, Val: 0.8085, Test: 0.7608
Epoch: 030, Loss: 0.5120, acc: 0.8371, hamming_loss: 0.1629, Val: 0.8037, Test: 0.7608
Epoch: 031, Loss: 0.5183, acc: 0.8309, hamming_loss: 0.1691, Val: 0.7988, Test: 0.7608
Epoch: 032, Loss: 0.5069, acc: 0.8429, hamming_loss: 0.1571, Val: 0.7977, Test: 0.7608
Epoch: 033, Loss: 0.5167, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8131, Test: 0.7608
Epoch: 034, Loss: 0.5084, acc: 0.8451, hamming_loss: 0.1549, Val: 0.8388, Test: 0.7608
Epoch: 035, Loss: 0.5062, acc: 0.8502, hamming_loss: 0.1498, Val: 0.8615, Test: 0.7816
Epoch: 036, Loss: 0.5044, acc: 0.8316, hamming_loss: 0.1684, Val: 0.8772, Test: 0.7895
Epoch: 037, Loss: 0.5111, acc: 0.8451, hamming_loss: 0.1549, Val: 0.8740, Test: 0.7895
Epoch: 038, Loss: 0.4992, acc: 0.8491, hamming_loss: 0.1509, Val: 0.8480, Test: 0.7895
Epoch: 039, Loss: 0.5064, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8264, Test: 0.7895
Epoch: 040, Loss: 0.5113, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8264, Test: 0.7895
Epoch: 041, Loss: 0.5133, acc: 0.8338, hamming_loss: 0.1662, Val: 0.8286, Test: 0.7895
Epoch: 042, Loss: 0.5083, acc: 0.8422, hamming_loss: 0.1578, Val: 0.8388, Test: 0.7895
Epoch: 043, Loss: 0.5053, acc: 0.8298, hamming_loss: 0.1702, Val: 0.8486, Test: 0.7895
Epoch: 044, Loss: 0.4976, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8486, Test: 0.7895
Epoch: 045, Loss: 0.4951, acc: 0.8520, hamming_loss: 0.1480, Val: 0.8378, Test: 0.7895
Epoch: 046, Loss: 0.5120, acc: 0.8451, hamming_loss: 0.1549, Val: 0.8491, Test: 0.7895
Epoch: 047, Loss: 0.5070, acc: 0.8462, hamming_loss: 0.1538, Val: 0.8496, Test: 0.7895
Epoch: 048, Loss: 0.5022, acc: 0.8499, hamming_loss: 0.1501, Val: 0.8378, Test: 0.7895
Epoch: 049, Loss: 0.5031, acc: 0.8480, hamming_loss: 0.1520, Val: 0.8323, Test: 0.7895
Epoch: 050, Loss: 0.5191, acc: 0.8426, hamming_loss: 0.1574, Val: 0.8437, Test: 0.7895
Epoch: 051, Loss: 0.5095, acc: 0.8499, hamming_loss: 0.1501, Val: 0.8453, Test: 0.7895
Epoch: 052, Loss: 0.5101, acc: 0.8345, hamming_loss: 0.1655, Val: 0.8442, Test: 0.7895
Epoch: 053, Loss: 0.5067, acc: 0.8389, hamming_loss: 0.1611, Val: 0.8518, Test: 0.7895
Epoch: 054, Loss: 0.5133, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8605, Test: 0.7895
Epoch: 055, Loss: 0.5027, acc: 0.8458, hamming_loss: 0.1542, Val: 0.8534, Test: 0.7895
Epoch: 056, Loss: 0.5037, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8394, Test: 0.7895
Epoch: 057, Loss: 0.5053, acc: 0.8557, hamming_loss: 0.1443, Val: 0.8432, Test: 0.7895
Epoch: 058, Loss: 0.5000, acc: 0.8517, hamming_loss: 0.1483, Val: 0.8415, Test: 0.7895
Epoch: 059, Loss: 0.5015, acc: 0.8546, hamming_loss: 0.1454, Val: 0.8361, Test: 0.7895
Epoch: 060, Loss: 0.5050, acc: 0.8448, hamming_loss: 0.1552, Val: 0.8367, Test: 0.7895
Epoch: 061, Loss: 0.4996, acc: 0.8499, hamming_loss: 0.1501, Val: 0.8394, Test: 0.7895
Epoch: 062, Loss: 0.5207, acc: 0.8389, hamming_loss: 0.1611, Val: 0.8432, Test: 0.7895
Epoch: 063, Loss: 0.5063, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8464, Test: 0.7895
Epoch: 064, Loss: 0.5164, acc: 0.8360, hamming_loss: 0.1640, Val: 0.8518, Test: 0.7895
Epoch: 065, Loss: 0.5013, acc: 0.8550, hamming_loss: 0.1450, Val: 0.8502, Test: 0.7895
Epoch: 066, Loss: 0.5058, acc: 0.8520, hamming_loss: 0.1480, Val: 0.8496, Test: 0.7895
Epoch: 067, Loss: 0.5387, acc: 0.8236, hamming_loss: 0.1764, Val: 0.8475, Test: 0.7895
Epoch: 068, Loss: 0.5162, acc: 0.8345, hamming_loss: 0.1655, Val: 0.8561, Test: 0.7895
Epoch: 069, Loss: 0.5043, acc: 0.8502, hamming_loss: 0.1498, Val: 0.8394, Test: 0.7895
Epoch: 070, Loss: 0.4931, acc: 0.8495, hamming_loss: 0.1505, Val: 0.8264, Test: 0.7895
Epoch: 071, Loss: 0.5067, acc: 0.8422, hamming_loss: 0.1578, Val: 0.8210, Test: 0.7895
Epoch: 072, Loss: 0.5073, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8340, Test: 0.7895
Epoch: 073, Loss: 0.4997, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8291, Test: 0.7895
Epoch: 074, Loss: 0.4912, acc: 0.8663, hamming_loss: 0.1337, Val: 0.8291, Test: 0.7895
Epoch: 075, Loss: 0.5091, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8464, Test: 0.7895
Epoch: 076, Loss: 0.4887, acc: 0.8673, hamming_loss: 0.1327, Val: 0.8605, Test: 0.7895
Epoch: 077, Loss: 0.4989, acc: 0.8440, hamming_loss: 0.1560, Val: 0.8610, Test: 0.7895
Epoch: 078, Loss: 0.5066, acc: 0.8466, hamming_loss: 0.1534, Val: 0.8480, Test: 0.7895
Epoch: 079, Loss: 0.4979, acc: 0.8531, hamming_loss: 0.1469, Val: 0.8194, Test: 0.7895
Epoch: 080, Loss: 0.4937, acc: 0.8560, hamming_loss: 0.1440, Val: 0.7923, Test: 0.7895
Epoch: 081, Loss: 0.5044, acc: 0.8477, hamming_loss: 0.1523, Val: 0.7988, Test: 0.7895
Epoch: 082, Loss: 0.5084, acc: 0.8535, hamming_loss: 0.1465, Val: 0.8442, Test: 0.7895
Epoch: 083, Loss: 0.4964, acc: 0.8302, hamming_loss: 0.1698, Val: 0.8643, Test: 0.7895
Epoch: 084, Loss: 0.5030, acc: 0.8433, hamming_loss: 0.1567, Val: 0.8632, Test: 0.7895
Epoch: 085, Loss: 0.4873, acc: 0.8484, hamming_loss: 0.1516, Val: 0.8664, Test: 0.7895
Epoch: 086, Loss: 0.4958, acc: 0.8473, hamming_loss: 0.1527, Val: 0.8551, Test: 0.7895
Epoch: 087, Loss: 0.4876, acc: 0.8560, hamming_loss: 0.1440, Val: 0.8134, Test: 0.7895
Epoch: 088, Loss: 0.4980, acc: 0.8437, hamming_loss: 0.1563, Val: 0.7956, Test: 0.7895
Epoch: 089, Loss: 0.4998, acc: 0.8495, hamming_loss: 0.1505, Val: 0.8102, Test: 0.7895
Epoch: 090, Loss: 0.4961, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8383, Test: 0.7895
Epoch: 091, Loss: 0.4954, acc: 0.8426, hamming_loss: 0.1574, Val: 0.8513, Test: 0.7895
Epoch: 092, Loss: 0.4916, acc: 0.8411, hamming_loss: 0.1589, Val: 0.8578, Test: 0.7895
Epoch: 093, Loss: 0.4970, acc: 0.8437, hamming_loss: 0.1563, Val: 0.8529, Test: 0.7895
Epoch: 094, Loss: 0.4901, acc: 0.8448, hamming_loss: 0.1552, Val: 0.8383, Test: 0.7895
Epoch: 095, Loss: 0.5033, acc: 0.8502, hamming_loss: 0.1498, Val: 0.8356, Test: 0.7895
Epoch: 096, Loss: 0.4884, acc: 0.8437, hamming_loss: 0.1563, Val: 0.8421, Test: 0.7895
Epoch: 097, Loss: 0.4802, acc: 0.8648, hamming_loss: 0.1352, Val: 0.8513, Test: 0.7895
Epoch: 098, Loss: 0.4906, acc: 0.8455, hamming_loss: 0.1545, Val: 0.8518, Test: 0.7895
Epoch: 099, Loss: 0.5127, acc: 0.8400, hamming_loss: 0.1600, Val: 0.8524, Test: 0.7895
Epoch: 100, Loss: 0.4836, acc: 0.8488, hamming_loss: 0.1512, Val: 0.8399, Test: 0.7895
Backend QtAgg is interactive backend. Turning interactive mode on.

